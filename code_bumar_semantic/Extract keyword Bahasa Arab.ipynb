{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from cltk.stop.arabic.stopword_filter import stopwords_filter as ar_stop_filter\n",
    "from string import digits\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "    # Normalisasi Text\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return(text)\n",
    "    # Remove Punctuations\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(\"`\", \"\", text)\n",
    "    text = re.sub(\"÷\", \"\", text)\n",
    "    text = re.sub(\"×\", \"\", text)\n",
    "    text = re.sub(\"_\", \"\", text)\n",
    "    text = re.sub(\"-\", \"\", text)\n",
    "    text = re.sub(\"“\", \"\", text)\n",
    "    text = re.sub(\"…\", \"\", text)\n",
    "    text = re.sub(\"!\", \"\", text)\n",
    "    text = re.sub(\"|\", \"\", text)\n",
    "    text = re.sub(\"¦\", \"\", text)\n",
    "    text = re.sub(\"~\", \"\", text)\n",
    "    text = re.sub(\"{\", \"\", text)\n",
    "    text = re.sub(\"}\", \"\", text)\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    text = re.sub(\"\\.\", \"\", text)\n",
    "    text = re.sub(\",\", \"\", text)\n",
    "    text = re.sub(\"/\", \"\", text)\n",
    "    text = re.sub(\":\", \"\", text)\n",
    "    text = re.sub(\",\", \"\", text)\n",
    "    text = re.sub(\"%\", \"\", text)\n",
    "    text = re.sub(\"^\", \"\", text)\n",
    "    text = re.sub(\"&\", \"\", text)\n",
    "    text = re.sub(\"^\", \"\", text)\n",
    "    text = re.sub(\"\\*\", \"\", text)\n",
    "    text = re.sub(\"،\", \"\", text)\n",
    "    text = re.sub(\"\\)\", \"\", text)\n",
    "    text = re.sub(\"\\(\", \"\", text)\n",
    "    text = re.sub(\">\", \"\", text)\n",
    "    text = re.sub(\"<\", \"\", text)\n",
    "    text = re.sub(\"؛\", \"\", text)\n",
    "    text = re.sub(\"؟\", \"\", text)\n",
    "    text = re.sub(\":\", \"\", text)\n",
    "    text = re.sub(\"\\\"\", \"\", text)\n",
    "    text = re.sub(\"\\[\", \"\", text)\n",
    "    text = re.sub(\"\\]\", \"\", text)\n",
    "    text = re.sub(\"\\+\", \"\", text)\n",
    "    text = re.sub(\"»\", \"\", text)\n",
    "    text = re.sub(\"«\", \"\", text)\n",
    "    return text\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "def remove_digit(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    return text\n",
    "def stemming(text):\n",
    "    hasil=[]\n",
    "    for i in text:\n",
    "        textHasil= st.stem(i)\n",
    "        hasil.append(textHasil)\n",
    "    return hasil\n",
    "\n",
    "def preprocessing (text):\n",
    "    text = remove_diacritics(text)\n",
    "    text = normalizeArabic(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_digit(text)\n",
    "    text = ar_stop_filter(text)\n",
    "    text=stemming(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "باب فرض وضء وسن وها فرض وضء ست خصل نيه عمند غسل وجه غسل وجه غسل ذرع رفق مسح قل راس غسل رجل كعب ريب وعل قول ولء وسن عشر خصل خمس غسل وجه وهي سمه غسل كفن ضمض نشق بلغ فيه الا صءم خمس غسل وجه وهي قدم يمن علي يسر مسح جمع راس مسح اذن ظهر وبط دخل صبع فيه خلل صبع رجل غسل دخل كعب وليس مسح عنق سنه فضل كرر ثلث وزالواجب مره مرت فضل ثلث كمل وها ان يبد طهر عضء وضع بدء فان قصر علي فرض است جزه وان ضيع حظ نفس ترك\n"
     ]
    }
   ],
   "source": [
    "text = 'بَاب فرض الْوضُوء وسننه وهيآته وَفرض الْوضُوء سِتّ خِصَال النِّيَّة عمند غسل الْوَجْه وَغسل الْوَجْه وَغسل الذراعين مَعَ الْمرْفقين وَمسح مَا قل من الرَّأْس وَغسل الرجلَيْن مَعَ الْكَعْبَيْنِ وَالتَّرْتِيب وعَلى قَول الْوَلَاء وسننه عشر خِصَال خمس مِنْهَا قبل غسل الْوَجْه وَهِي التَّسْمِيَة وَغسل الْكَفَّيْنِ والمضمضة وَالِاسْتِنْشَاق وَالْمُبَالغَة فيههما إِلَّا للصَّائِم وَخمْس بعد غسل الْوَجْه وَهِي تَقْدِيم الْيُمْنَى على ليسرى وَمسح جَمِيع الرَّأْس وَمسح الْأُذُنَيْنِ ظاهرهما وباطنهما وَإِدْخَال الأصبعين فيهمَا وتخليل أَصَابِع الرجلَيْن وَغسل دَاخل الْكَعْبَيْنِ وَلَيْسَ مسح لعنق من سنَنه وفضيلته تكراره ثَلَاثًا وزالواجب فِيهِ مرّة والمرتان أفضل وَالثَّلَاث أكمل وهيآته أَن يبْدَأ فِي تَطْهِير الْأَعْضَاء بمواضع الِابْتِدَاء فَإِن اقْتصر على فروضه استة أَجزَأَهُ وَإِن ضيع حَظّ نَفسه فِيمَا ترك'\n",
    "text=preprocessing(text)\n",
    "doc=\" \".join(text)\n",
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of article\n",
      " غسل\n",
      "مسح\n",
      "فرض\n",
      "علي\n",
      "فضل\n",
      "ثلث\n",
      "وسن وها\n",
      "فيه\n",
      "خصل\n"
     ]
    }
   ],
   "source": [
    "#Keywords Extraction with TextRank\n",
    "\n",
    "from summa import keywords\n",
    "print(\"Keywords of article\\n\", (keywords.keywords(doc,words=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of article\n",
      " ['غسل', 'وجه', 'مسح', 'فرض', 'وضء', 'خمس', 'وسن', 'وها', 'خصل', 'راس']\n"
     ]
    }
   ],
   "source": [
    "#Keywords Extraction with YAKE\n",
    "\n",
    "from yake import KeywordExtractor\n",
    "kw_extractor = KeywordExtractor(lan=\"ar\", n=1, top=10)\n",
    "#for j in range(len(array_text)):\n",
    "keywords = kw_extractor.extract_keywords(text=doc)\n",
    "keywords = [x for x, y in keywords]\n",
    "print(\"Keywords of article\\n\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dfd451e70ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#for j in range(len(array_text)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Keywords of article\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Keywords Extraction with BERT\n",
    "\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "#for j in range(len(array_text)):\n",
    "keywords = kw_extractor.extract_keywords(doc, top_n=10, keyphrase_ngram_range=(3, 3))\n",
    "print(\"Keywords of article\\n\", keywords)\n",
    "for i in range (0,len (keywords)):\n",
    "    print (keywords[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وزالواجب\n",
      "ثلث وزالواجب\n",
      "جزه\n",
      "غسل وجه\n",
      "فرض\n",
      "وزالواجب مره\n",
      "فضل\n",
      "وجه غسل\n",
      "رجل\n",
      "غسل كفن\n",
      "غسل\n",
      "غسل ذرع\n",
      "وجه\n",
      "رجل غسل\n",
      "رفق\n",
      "كفن ضمض\n",
      "ذرع\n",
      "غسل رجل\n",
      "يبد\n",
      "رجل كعب\n"
     ]
    }
   ],
   "source": [
    "keyword1 = kw_extractor.extract_keywords(doc, top_n=10, keyphrase_ngram_range=(1, 1))\n",
    "keyword2 = kw_extractor.extract_keywords(doc, top_n=10, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "#print(\"Keywords of article\\n\", keywords)\n",
    "for i in range (0,len (keyword1)):\n",
    "    print (keyword1[i][0])\n",
    "    print (keyword2[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "3\n",
      "{'الْوَجْه': 0.10619469026548672, 'وَغسل': 0.08849557522123894, 'وَمسح': 0.07964601769911504, 'وسننه': 0.05309734513274336, 'غسل': 0.05309734513274336}\n"
     ]
    }
   ],
   "source": [
    "## TFIDF\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "doc = 'بَاب فرض الْوضُوء وسننه وهيآته وَفرض الْوضُوء سِتّ خِصَال النِّيَّة عمند غسل الْوَجْه وَغسل الْوَجْه وَغسل الذراعين مَعَ الْمرْفقين وَمسح مَا قل من الرَّأْس وَغسل الرجلَيْن مَعَ الْكَعْبَيْنِ وَالتَّرْتِيب وعَلى قَول الْوَلَاء وسننه عشر خِصَال خمس مِنْهَا قبل غسل الْوَجْه وَهِي التَّسْمِيَة وَغسل الْكَفَّيْنِ والمضمضة وَالِاسْتِنْشَاق وَالْمُبَالغَة فيههما إِلَّا للصَّائِم وَخمْس بعد غسل الْوَجْه وَهِي تَقْدِيم الْيُمْنَى على ليسرى وَمسح جَمِيع الرَّأْس وَمسح الْأُذُنَيْنِ ظاهرهما وباطنهما وَإِدْخَال الأصبعين فيهمَا وتخليل أَصَابِع الرجلَيْن . وَغسل دَاخل الْكَعْبَيْنِ وَلَيْسَ مسح لعنق من سنَنه وفضيلته تكراره ثَلَاثًا وزالواجب فِيهِ مرّة والمرتان أفضل وَالثَّلَاث أكمل وهيآته أَن يبْدَأ فِي تَطْهِير الْأَعْضَاء بمواضع الِابْتِدَاء . فَإِن اقْتصر على فروضه استة أَجزَأَهُ وَإِن ضيع حَظّ نَفسه فِيمَا ترك @'\n",
    "total_words = re.sub(r'[^\\w]', ' ', doc)\n",
    "total_words = doc.lower().split()\n",
    "total_word_length = len(total_words)\n",
    "print(total_word_length)\n",
    "total_sentences = tokenize.sent_tokenize(doc)\n",
    "total_sent_len = len(total_sentences)\n",
    "print(total_sent_len)\n",
    "\n",
    "tf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "\n",
    "# Dividing by total_word_length for each dictionary element\n",
    "tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "#print(tf_score)\n",
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))\n",
    "\n",
    "idf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "# Performing a log and divide\n",
    "#idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "#print(idf_score)\n",
    "tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "#print(tf_idf_score)\n",
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result\n",
    "print(get_top_n(tf_idf_score, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9801.0,\n",
       "  'باب فرض وضء وسن وها فرض وضء ست خصل نيه عمند غسل وجه غسل وجه غسل ذرع رفق مسح قل راس غسل رجل كعب ريب وعل قول ولء وسن عشر خصل خمس غسل وجه وهي سمه غسل كفن ضمض نشق بلغ فيه الا صءم خمس غسل وجه وهي قدم يمن علي يسر مسح جمع راس مسح اذن ظهر وبط دخل صبع فيه خلل صبع رجل غسل دخل كعب وليس مسح عنق سنه فضل كرر ثلث وزالواجب مره مرت فضل ثلث كمل وها ان يبد طهر عضء وضع بدء فان قصر علي فرض است جزه وان ضيع حظ نفس ترك')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "\n",
    "r.extract_keywords_from_text(doc)\n",
    "\n",
    "r.get_ranked_phrases_with_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytopicrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7f33a08aed4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Keywords Extraction with TopicRank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytopicrank\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTopicRank\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTopicRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytopicrank'"
     ]
    }
   ],
   "source": [
    "#Keywords Extraction with TopicRank\n",
    "\n",
    "from pytopicrank import TopicRank\n",
    "for j in range(len(array_text)):\n",
    "    tr = TopicRank(array_text[j])\n",
    "    print(\"Keywords of article\", str(j+1), \"\\n\", tr.get_top_n(n=5, extract_strategy='first'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
